{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook demonstrates the use of InFoRM algorithms to mitigate bias for LINE embedding\n",
    "InFoRM includes 3 algorithms, namely debiasing the input graph, debiasing the mining model and debiasing the mining result. We will show how to run all 3 algorithms for LINE in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get vanilla embedding matrix first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load necessary packages\n",
    "import pickle\n",
    "import load_graph\n",
    "import utils\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import sklearn.preprocessing as skpp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from vanilla_line import *\n",
    "class LINE:\n",
    "    def __init__(self, dimension=128, ratio=3200, negative=5, batch_size=1000, init_lr=0.025, seed=None):\n",
    "        self.dimension = dimension\n",
    "        self.ratio = ratio\n",
    "        self.negative = negative\n",
    "        self.batch_size = batch_size\n",
    "        self.init_lr = init_lr\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "    def train(self, graph):\n",
    "        self.graph = graph\n",
    "        self.is_directed = nx.is_directed(self.graph)\n",
    "        self.num_node = graph.number_of_nodes()\n",
    "        self.num_sampling_edge = self.ratio * self.num_node\n",
    "\n",
    "        node2id = dict([(node, vid) for vid, node in enumerate(graph.nodes())])\n",
    "        self.edges = [[node2id[e[0]], node2id[e[1]]] for e in self.graph.edges()]\n",
    "        self.edges_prob = np.asarray([graph[u][v].get(\"weight\", 1.0) for u, v in graph.edges()])\n",
    "        self.edges_prob /= np.sum(self.edges_prob)\n",
    "        self.edges_table, self.edges_prob = alias_setup(self.edges_prob)\n",
    "\n",
    "        degree_weight = np.asarray([0] * self.num_node)\n",
    "        for u, v in graph.edges():\n",
    "            degree_weight[node2id[u]] += graph[u][v].get(\"weight\", 1.0)\n",
    "            if not self.is_directed:\n",
    "                degree_weight[node2id[v]] += graph[u][v].get(\"weight\", 1.0)\n",
    "        self.node_prob = np.power(degree_weight, 0.75)\n",
    "        self.node_prob /= np.sum(self.node_prob)\n",
    "        self.node_table, self.node_prob = alias_setup(self.node_prob)\n",
    "\n",
    "        self.emb_vertex = (np.random.random((self.num_node, self.dimension)) - 0.5) / self.dimension\n",
    "        self.fair_emb_vertex = self.emb_vertex.copy()\n",
    "        self._train_line()\n",
    "        self.embeddings = skpp.normalize(self.emb_vertex, \"l2\")\n",
    "        return self.embeddings\n",
    "\n",
    "    def _update(self, vec_u, vec_v, vec_error, label):\n",
    "        # update vetex embedding and vec_error\n",
    "        f = 1 / (1 + np.exp(-np.sum(vec_u * vec_v, axis=1)))\n",
    "        g = (self.lr * (label - f)).reshape((len(label), 1))\n",
    "        vec_error += g * vec_v\n",
    "        vec_v += g * vec_u\n",
    "\n",
    "    def _train_line(self):\n",
    "        self.lr = self.init_lr\n",
    "        batch_size = self.batch_size\n",
    "        num_batch = int(self.num_sampling_edge / batch_size)\n",
    "        epoch_iter = range(num_batch)\n",
    "        for b in epoch_iter:\n",
    "            # if b % self.batch_size == 0:\n",
    "            #     self.lr = self.init_lr * max((1 - b * 1.0 / num_batch), 0.0001)\n",
    "            self.lr = self.init_lr * max((1 - b * 1.0 / num_batch), 0.0001)\n",
    "            u, v = [0] * batch_size, [0] * batch_size\n",
    "            for i in range(batch_size):\n",
    "                edge_id = alias_draw(self.edges_table, self.edges_prob)\n",
    "                u[i], v[i] = self.edges[edge_id]\n",
    "                if not self.is_directed and np.random.rand() > 0.5:\n",
    "                    v[i], u[i] = self.edges[edge_id]\n",
    "\n",
    "            vec_error = np.zeros((batch_size, self.dimension))\n",
    "            label, target = np.asarray([1 for i in range(batch_size)]), np.asarray(v)\n",
    "            for j in range(self.negative + 1):\n",
    "                if j != 0:\n",
    "                    label = np.asarray([0 for i in range(batch_size)])\n",
    "                    for i in range(batch_size):\n",
    "                        target[i] = alias_draw(self.node_table, self.node_prob)\n",
    "                self._update(\n",
    "                    self.emb_vertex[u], self.emb_vertex[target], vec_error, label\n",
    "                )\n",
    "            self.emb_vertex[u] += vec_error\n",
    "\n",
    "def alias_setup(probs):\n",
    "    \"\"\"\n",
    "    Compute utility lists for non-uniform sampling from discrete distributions.\n",
    "    Refer to https://hips.seas.harvard.edu/blog/2013/03/03/the-alias-method-efficient-sampling-with-many-discrete-outcomes/\n",
    "    for details\n",
    "    \"\"\"\n",
    "    K = len(probs)\n",
    "    q = np.zeros(K)\n",
    "    J = np.zeros(K, dtype=np.int)\n",
    "\n",
    "    smaller = []\n",
    "    larger = []\n",
    "    for kk, prob in enumerate(probs):\n",
    "        q[kk] = K * prob\n",
    "        if q[kk] < 1.0:\n",
    "            smaller.append(kk)\n",
    "        else:\n",
    "            larger.append(kk)\n",
    "\n",
    "    while len(smaller) > 0 and len(larger) > 0:\n",
    "        small = smaller.pop()\n",
    "        large = larger.pop()\n",
    "\n",
    "        J[small] = large\n",
    "        q[large] = q[large] + q[small] - 1.0\n",
    "        if q[large] < 1.0:\n",
    "            smaller.append(large)\n",
    "        else:\n",
    "            larger.append(large)\n",
    "\n",
    "    return J, q\n",
    "\n",
    "\n",
    "def alias_draw(J, q):\n",
    "    \"\"\"\n",
    "    Draw sample from a non-uniform discrete distribution using alias sampling.\n",
    "    \"\"\"\n",
    "    K = len(J)\n",
    "\n",
    "    kk = int(np.floor(np.random.rand() * K))\n",
    "    if np.random.rand() < q[kk]:\n",
    "        return kk\n",
    "    else:\n",
    "        return J[kk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla(name, ratio, seed):\n",
    "    try:\n",
    "        with open('result/line/vanilla.pickle', 'rb') as f:\n",
    "            edict = pickle.load(f)\n",
    "    except:\n",
    "        edict = dict()\n",
    "\n",
    "    data = load_graph.read_pickle(name)\n",
    "    adj_train = data['adjacency_train']\n",
    "    graph = nx.from_scipy_sparse_matrix(adj_train, create_using=nx.Graph(), edge_attribute='weight')\n",
    "\n",
    "    # train\n",
    "    model = LINE(ratio=ratio, seed=seed)\n",
    "    edict[name] = model.train(graph)\n",
    "\n",
    "    with open('result/line/vanilla.pickle'.format(ratio, seed), 'wb') as f:\n",
    "        pickle.dump(edict, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "vanilla(name='ppi', ratio=3200, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's debias the input graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load debias model\n",
    "from method.debias_graph import DebiasGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debias_input_graph(name, alpha=0., lr=0., metric=None):\n",
    "    # load dataset\n",
    "    data = load_graph.read_pickle(name)\n",
    "    init_adj = data['adjacency_train']\n",
    "\n",
    "    # build similarity matrix\n",
    "    sim = utils.filter_similarity_matrix(utils.get_similarity_matrix(init_adj, metric=metric), sigma=0.75)\n",
    "\n",
    "    # debias LINE\n",
    "    FairGraph = DebiasGraph()\n",
    "    adj = FairGraph.line(init_adj, sim, alpha, maxiter=100, lr=lr, tol=1e-6)\n",
    "    graph = nx.from_scipy_sparse_matrix(adj, create_using=nx.Graph(), edge_attribute='weight')\n",
    "    model = LINE(ratio=3200, seed=0)\n",
    "    embs = model.train(graph)\n",
    "\n",
    "    print('dataset: {}\\tmetric: {} similarity'.format(name, metric))\n",
    "    print('Finished!')\n",
    "\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: ppi\tmetric: jaccard similarity\n",
      "Finished!\n",
      "dataset: ppi\tmetric: cosine similarity\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "# jaccard index\n",
    "result = dict()\n",
    "result['ppi'] = debias_input_graph('ppi', alpha=10, lr=0.025, metric='jaccard')\n",
    "with open('result/line/graph/jaccard.pickle', 'wb') as f:\n",
    "    pickle.dump(result, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# cosine similarity    \n",
    "result = dict()\n",
    "result['ppi'] = debias_input_graph('ppi', alpha=10, lr=0.025, metric='cosine')\n",
    "with open('result/line/graph/cosine.pickle', 'wb') as f:\n",
    "    pickle.dump(result, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's debias the mining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load debias model\n",
    "from method.debias_model import DebiasModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debias_mining_model(name, alpha=0., metric=None):\n",
    "    # load dataset\n",
    "    data = load_graph.read_pickle(name)\n",
    "    adj_train = data['adjacency_train']\n",
    "    graph = nx.from_scipy_sparse_matrix(adj_train, create_using=nx.Graph(), edge_attribute='weight')\n",
    "\n",
    "    # build similarity matrix\n",
    "    sim = utils.filter_similarity_matrix(utils.get_similarity_matrix(adj_train, metric=metric), sigma=0.75)\n",
    "\n",
    "    # debias LINE\n",
    "    FairModel = DebiasModel()\n",
    "    embs = FairModel.line(\n",
    "        graph, sim, alpha,\n",
    "        dimension=128, ratio=3200, negative=5,\n",
    "        init_lr=0.025, batch_size=1000, seed=0\n",
    "    )\n",
    "\n",
    "    print('dataset: {}\\tmetric: {} similarity'.format(name, metric))\n",
    "    print('Finished!')\n",
    "\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: ppi\tmetric: jaccard similarity\n",
      "Finished!\n",
      "dataset: ppi\tmetric: cosine similarity\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.5\n",
    "# jaccard index\n",
    "result = dict()\n",
    "result['ppi'] = debias_mining_model(name='ppi', alpha=alpha, metric='jaccard')\n",
    "with open('result/line/model/jaccard.pickle', 'wb') as f:\n",
    "    pickle.dump(result, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# cosine similarity    \n",
    "result = dict()\n",
    "result['ppi'] = debias_mining_model(name='ppi', alpha=alpha, metric='cosine')\n",
    "with open('result/line/model/cosine.pickle', 'wb') as f:\n",
    "    pickle.dump(result, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's debias the mining result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load debias model\n",
    "from method.debias_result import DebiasResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debias_mining_result(name, vanilla, alpha=0., metric=None):\n",
    "    # vanilla embeddings\n",
    "    embs = vanilla[name]\n",
    "\n",
    "    # load dataset\n",
    "    data = load_graph.read_pickle(name)\n",
    "    adj_train = data['adjacency_train']\n",
    "\n",
    "    # build similarity matrix\n",
    "    sim = utils.filter_similarity_matrix(utils.get_similarity_matrix(adj_train, metric=metric), sigma=0.75)\n",
    "\n",
    "    # debias LINE\n",
    "    FairResult = DebiasResult()\n",
    "    embs = FairResult.fit(embs, sim, alpha)\n",
    "\n",
    "    print('dataset: {}\\tmetric: {} similarity'.format(name, metric))\n",
    "    print()\n",
    "\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: ppi\tmetric: jaccard similarity\n",
      "\n",
      "dataset: ppi\tmetric: cosine similarity\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.5\n",
    "with open('result/line/vanilla.pickle', 'rb') as f:\n",
    "    vanilla = pickle.load(f)\n",
    "\n",
    "# jaccard index\n",
    "result = dict()\n",
    "result['ppi'] = debias_mining_result(name='ppi', vanilla=vanilla, alpha=alpha, metric='jaccard')\n",
    "with open('result/line/result/jaccard.pickle', 'wb') as f:\n",
    "    pickle.dump(result, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# cosine similarity    \n",
    "result = dict()\n",
    "result['ppi'] = debias_mining_result(name='ppi', vanilla=vanilla, alpha=alpha, metric='cosine')\n",
    "with open('result/line/result/cosine.pickle', 'wb') as f:\n",
    "    pickle.dump(result, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's see how much we debiased and how good debiased results are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load evaluation functions\n",
    "from evaluate.line import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'ppi', 'metric': 'jaccard similarity', 'task': 'debias the input graph', 'diff': 0.6740087760748081, 'roc-auc': (0.6818451361318293, 0.6784420320651908), 'f1': (0.6178672863413375, 0.6201910663568293), 'bias': {'train': [0.020624031681280353], 'validation': [0.03629812978226654], 'test': [0.015421255425152935]}}\n",
      "{'dataset': 'ppi', 'metric': 'cosine similarity', 'task': 'debias the input graph', 'diff': 0.6993078976931937, 'roc-auc': (0.6818451361318293, 0.6855929039010265), 'f1': (0.6178672863413375, 0.6207074619158275), 'bias': {'train': [0.012206033740946975], 'validation': [0.03451971868807613], 'test': [0.02550044009603536]}}\n",
      "{'dataset': 'ppi', 'metric': 'jaccard similarity', 'task': 'debias the mining model', 'diff': 0.23823116757757387, 'roc-auc': (0.6818451361318293, 0.7154187140657258), 'f1': (0.6178672863413375, 0.6423960753937517), 'bias': {'train': [0.05852995194383237], 'validation': [0.25737305180892533], 'test': [0.2644720912112759]}}\n",
      "{'dataset': 'ppi', 'metric': 'cosine similarity', 'task': 'debias the mining model', 'diff': 0.4175146072467481, 'roc-auc': (0.6818451361318293, 0.7401281682310865), 'f1': (0.6178672863413375, 0.6692486444616577), 'bias': {'train': [0.07714150492578331], 'validation': [0.344702801397869], 'test': [0.34973556910429104]}}\n",
      "{'dataset': 'ppi', 'metric': 'jaccard similarity', 'task': 'debias the mining result', 'diff': 0.507761363122116, 'roc-auc': (0.6818451361318293, 0.7132877990582615), 'f1': (0.6178672863413375, 0.6423960753937517), 'bias': {'train': [0.9007993898804335], 'validation': [0.8480163338139572], 'test': [0.8705919014053077]}}\n",
      "{'dataset': 'ppi', 'metric': 'cosine similarity', 'task': 'debias the mining result', 'diff': 0.7220286242529881, 'roc-auc': (0.6818451361318293, 0.6343595445079172), 'f1': (0.6178672863413375, 0.5886909372579395), 'bias': {'train': [0.970455384636739], 'validation': [0.958253913214724], 'test': [0.9627412770400863]}}\n"
     ]
    }
   ],
   "source": [
    "evaluate(name='ppi', metric='jaccard', task='graph')\n",
    "evaluate(name='ppi', metric='cosine', task='graph')\n",
    "evaluate(name='ppi', metric='jaccard', task='model')\n",
    "evaluate(name='ppi', metric='cosine', task='model')\n",
    "evaluate(name='ppi', metric='jaccard', task='result')\n",
    "evaluate(name='ppi', metric='cosine', task='result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
